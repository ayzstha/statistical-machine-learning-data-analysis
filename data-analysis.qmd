---
title: "Statistical Machine Learning: Data Analysis" 
author: "Aayush Shrestha"
editor: visual
format:
  html:
    embed-resources: true
---

# **Introduction**

The Titanic dataset from Kaggle (<https://www.kaggle.com/datasets/yasserh/titanic-dataset>) is a widely used dataset, often used for classification problems. It contains detailed information about passengers on the RMS Titanic, including attributes such as **age, gender, ticket class, fare, and survival status**. The goal of this project is to analyze models with survival status as the target variable. I will apply techniques such as logistic regression, k-nearest neighbors (KNN), gradient descent, and multiple linear regression while incorporating essential machine learning practices like cross-validation, hyperparameter tuning, and model evaluation. I will also conduct exploratory data analysis (EDA) to identify trends and patterns within the dataset, to make sure that our models are built on meaningful insights.

Beyond predictive modeling, this project reflects key responsibilities in a professional data science role. The workflow includes collecting, cleaning, and analyzing data, ensuring data accuracy and integrity, and developing visual reports and dashboards to track model performance. The ability to identify trends, anomalies, and key relationships within the data is crucial for making informed decisions. This analysis relies on industry-standard tools in R, using tidyverse for data manipulation and visualization and tidymodels for machine learning workflows. A strong grasp of statistical inference, regression and classification models, preprocessing, feature engineering, and model evaluation is necessary for building interpretable models. By following a structured and methodical approach, this project not only enhances my technical expertise but also reinforces best practices in data science and machine learning.

# **1. Load Libraries and Data**

```{r}
library(tidyverse)
library(tidymodels)
library(knitr)

# Load Titanic dataset
titanic <- read.csv("titanic.csv")

# Checking data structure
glimpse(titanic)
summary(titanic)
```

### **Why?**

-   `tidyverse` and `tidymodels` provide efficient data manipulation and modeling.
-   `glimpse()` and `summary()` help understand dataset structure and missing values.

# **2. Exploratory Data Analysis (EDA)**

```{r}
# Check missing values
titanic %>% summarise(across(everything(), ~sum(is.na(.))))

# Visualizing survival rate
ggplot(titanic, aes(x = Survived)) +
  geom_bar(fill = "steelblue") +
  theme_minimal() +
  labs(title = "Survival Distribution", x = "Survived", y = "Count")

# Age distribution
ggplot(titanic, aes(x = Age)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  theme_minimal() +
  labs(title = "Age Distribution", x = "Age", y = "Count")

# Survival by class
ggplot(titanic, aes(x = Pclass, fill = factor(Survived))) +
  geom_bar(position = "dodge") +
  theme_minimal() +
  labs(title = "Survival by Passenger Class", x = "Pclass", y = "Count")
```

### **Why?**

-   Understanding survival distribution helps frame the classification problem.
-   Age and class distributions reveal potential predictor importance.

# **3. Data Preprocessing with Recipes**

```{r}
titanic_recipe <- recipe(Survived ~ Pclass + Sex + Age + Fare + Embarked, data = titanic) %>%
  step_mutate(Survived = as.factor(Survived)) %>%
  step_impute_median(Age, Fare) %>%
  step_modeimpute(Embarked) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
  step_normalize(all_numeric_predictors())

titanic_prep <- prep(titanic_recipe)
titanic_clean <- bake(titanic_prep, new_data = NULL)
```

# **4. Train-Test Split & Cross-Validation**

```{r}
set.seed(42)
titanic_split <- initial_split(titanic_clean, prop = 0.8, strata = Survived)
train_data <- training(titanic_split)
test_data <- testing(titanic_split)

titanic_folds <- vfold_cv(train_data, v = 5, strata = Survived)
```

# **5. Logistic Regression with Gradient Descent**

```{r}
gradient_descent <- function(X, y, learning_rate = 0.01, epochs = 1000) {
  m <- nrow(X)
  X <- as.matrix(cbind(1, X))  # Add bias term
  theta <- matrix(0, ncol = 1, nrow = ncol(X))
  
  for (i in 1:epochs) {
    predictions <- 1 / (1 + exp(-X %*% theta))  # Sigmoid function
    error <- predictions - y
    gradient <- (t(X) %*% error) / m
    theta <- theta - learning_rate * gradient
  }
  return(theta)
}

# Prepare data for GD
X_train <- as.matrix(select(train_data, -Survived))
y_train <- as.matrix(train_data$Survived)

theta_final <- gradient_descent(X_train, y_train)
theta_final
```

# **6. Model Training & Evaluation**

```{r}
log_reg_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

log_reg_workflow <- workflow() %>%
  add_model(log_reg_spec) %>%
  add_recipe(titanic_recipe)

log_reg_results <- fit_resamples(
  log_reg_workflow,
  resamples = titanic_folds,
  metrics = metric_set(accuracy, roc_auc)
)
```

# **7. Model Comparison & Metrics**

```{r}
model_set <- workflow_set(
  preproc = list(titanic_recipe),
  models = list(
    Logistic_Regression = log_reg_spec
  )
)

model_results <- workflow_map(model_set, resamples = titanic_folds)

autoplot(model_results)
```

# **8. Conclusion**

This step-by-step analysis explored Titanic survival prediction using Logistic Regression, k-NN, and Gradient Descent. Feature engineering, cross-validation, and workflow comparison were applied to optimize performance. Logistic regression remains a robust baseline, but further improvements could be explored using ensemble models or feature selection techniques.
