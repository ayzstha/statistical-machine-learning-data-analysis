---
title: "Statistical Machine Learning: Data Analysis" 
author: "Aayush Shrestha"
editor: visual
format:
  html:
    embed-resources: true
---

# **Introduction** 

The Titanic dataset from Kaggle (<https://www.kaggle.com/datasets/yasserh/titanic-dataset>) is a fascinating window into one of history's most famous maritime disasters. This is a widely used dataset, often used for classification problems. It contains detailed information about passengers on the RMS Titanic, including attributes such as **age, gender, ticket class, fare, and survival status**. The goal of this project is to analyze models with survival status as the target variable. I will apply techniques such as logistic regression, k-nearest neighbors (KNN), gradient descent, and multiple linear regression while incorporating essential machine learning practices like cross-validation, hyperparameter tuning, and model evaluation. I will also conduct exploratory data analysis (EDA) to identify trends and patterns within the dataset, to make sure that our models are built on meaningful insights.

Beyond predictive modeling, this project reflects key responsibilities in a professional data science role. The workflow includes collecting, cleaning, and analyzing data, ensuring data accuracy and integrity, and developing visual reports and dashboards to track model performance. The ability to identify trends, anomalies, and key relationships within the data is crucial for making informed decisions. This analysis relies on industry-standard tools in R, using tidyverse for data manipulation and visualization and tidymodels for machine learning workflows. A strong grasp of statistical inference, regression and classification models, preprocessing, feature engineering, and model evaluation is necessary for building interpretable models. By following a structured and methodical approach, this project not only enhances my technical expertise but also reinforces best practices in data science and machine learning.

# **1. Loading Libraries and Data**

```{r}
# Load Required Libraries
library(tidyverse)
library(tidymodels)
library(kknn)
library(ggrepel)
library(yardstick)
library(ggplot2)

# Set Seed for Reproducibility
set.seed(401)  

# "The Titanic is associated with the number 401 because that was the yard number assigned to the ship by its builders, Harland and Wolff". Just thought this was a fun seed number!

# Load Titanic Dataset
titanic <- read_csv("titanic.csv")

# View First Few Rows
glimpse(titanic)

```

We've loaded essential packages like tidyverse for data manipulation, tidymodels for building predictive workflows, and visualization tools like ggplot2. I've set our random seed to 401 which was the actual yard number assigned to the Titanic by its builders, adding a touch of historical significance while ensuring reproducible results.

Our initial glimpse at the dataset reveals 891 passenger records with various features. The target variable "Survived" (0 = perished, 1 = survived) will be what we're trying to predict. We have both numerical variables like Age (ranging from 0.42 to 80 years) and Fare (from free passage to a whopping 512.33), as well as categorical variables like passenger class and embarkation point. There are some missing values, particularly in Age (177 missing entries), which we'll need to address during preprocessing.

# **2. Cleaning Data**

```{r}
titanic <- titanic |>
  mutate(
    Survived = factor(Survived),
    Pclass = factor(Pclass),
    Sex = factor(Sex),
    Embarked = factor(Embarked)
  ) |>
  select(-PassengerId, -Name, -Ticket, -Cabin)  # Drop unnecessary columns


```

Here, I've converted categorical variables (Survived, Pclass, Sex, Embarked) into factors, making them suitable for our classification models. I've also removed less useful columns like PassengerId, Name, Ticket, and Cabin to focus on predictive features.

I've left missing values intact at this stage rather than imputing them immediately. This is to prevent data leakage. We will handle missing values within our modeling workflow using various imputation strategies to see which performs best.

# **3. Exploratory Data Analysis (EDA)**

```{r}
# Plot the distribution of numerical features
numerical_features <- c("Age", "SibSp", "Parch", "Fare")
for (feature in numerical_features) {
  print(
    ggplot(titanic, aes(x = .data[[feature]])) +
      geom_histogram(binwidth = 10, fill = "blue", color = "black") +
      ggtitle(paste("Distribution of", feature)) +
      theme_minimal() +
      theme(plot.title = element_text(hjust = 0.5)) +
      labs(x = feature, y = "Count")
  )
}

# Plot the distribution of categorical features
categorical_features <- c("Survived", "Pclass", "Sex", "Embarked")
for (feature in categorical_features) {
  print(
    ggplot(titanic, aes(x = .data[[feature]])) +
      geom_bar(fill = "blue", color = "black") +
      ggtitle(paste("Distribution of", feature)) +
      theme_minimal() +
      theme(plot.title = element_text(hjust = 0.5)) +
      labs(x = feature, y = "Count")
  )
}

# Plot survival rate by different features
features_to_plot <- c("Pclass", "Sex", "Embarked")
for (feature in features_to_plot) {
  print(
    ggplot(titanic, aes(x = .data[[feature]], fill = as.factor(Survived))) +
      geom_bar(position = "fill") +
      ggtitle(paste("Survival Rate by", feature)) +
      theme_minimal() +
      theme(plot.title = element_text(hjust = 0.5)) +
      labs(x = feature, y = "Proportion", fill = "Survived")
  )
}
```

Our EDA reveals some interesting but unsurprising realities about the Titanic disaster. The survival rate shows that significantly more passengers perished than survived.

Diving deeper (no pun intended) into demographics, we observe that passengers between 20-40 years had the highest survival rates relative to other age groups. The most striking pattern emerges when examining passenger class: third-class passengers had dramatically lower survival rates compared to first-class passengers, highlighting the socioeconomic difference which eventually influenced survival outcomes.

These visualizations provide crucial insights into potential feature importance for our predictive models and highlight data anomalies we'll need to address during preprocessing.

# **4. Train-Test Split & Cross-Validation**

```{r}
titanic_split <- initial_split(titanic, prop = 0.8, strata = Survived)
train_data <- training(titanic_split)
test_data <- testing(titanic_split)

# 10-Fold Cross-Validation with 10 repeats
titanic_folds <- vfold_cv(train_data, v = 10, strata = Survived, repeats = 10)

```

Here, we've partitioned our data into training (80%) and testing (20%) sets. I've used stratification to maintain the same proportion of survivors in both sets, preventing any artificial bias in our evaluation.

I've also implemented 10-fold cross-validation with 10 repeats. This rigorous approach means each model will be tested across 100 different training/validation combinations, giving us extremely reliable performance estimates and reducing the variance in our assessments.

# **5. Defining our models**

```{r}
# Logistic Regression
log_reg_model <- logistic_reg() |> 
  set_engine("glm") |> 
  set_mode("classification")

# KNN Models with Different k Values
knn5_model <- nearest_neighbor(neighbors = 5) |> 
  set_engine("kknn") |> 
  set_mode("classification")

knn10_model <- nearest_neighbor(neighbors = 10) |> 
  set_engine("kknn") |> 
  set_mode("classification")
```

For our prediction, I've selected two different types of models. First is logistic regression, which is reliable, interpretable, and effective for understanding feature importance. Then we have k-nearest neighbors (KNN) with two different configurations (k=5 and k=10), which is a more flexible approach and adapts to local patterns in the data.

Since we're predicting a binary outcome (survived or didn't survive), all models are set to "classification" mode rather than "regression." Each model has its strengths: logistic regression excels at providing interpretable odds ratios, while KNN can capture more complex, non-linear relationships in the data.

# **6. Preprocessing Data with Recipes**

```{r}
# Update the recipes to handle missing values in categorical variables
# For Logistic Regression recipes
logreg_meanimpute <- recipe(Survived ~ ., data = train_data) %>% 
  step_nzv(all_predictors()) %>%  
  step_zv(all_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>%
  step_unknown(Embarked) %>%  # Handle missing values in Embarked
  step_dummy(all_nominal_predictors(), one_hot = FALSE) %>%
  step_lincomb(all_numeric_predictors())  

logreg_medianimpute <- recipe(Survived ~ ., data = train_data) %>% 
  step_nzv(all_predictors()) %>%  
  step_zv(all_predictors()) %>%
  step_impute_median(all_numeric_predictors()) %>%  
  step_unknown(Embarked) %>%  # Handle missing values in Embarked
  step_dummy(all_nominal_predictors(), one_hot = FALSE) %>%
  step_lincomb(all_numeric_predictors())  

logreg_knnimpute <- recipe(Survived ~ ., data = train_data) %>% 
  step_nzv(all_predictors()) %>% 
  step_zv(all_predictors()) %>%
  step_impute_knn(all_numeric_predictors()) %>%  
  step_unknown(Embarked) %>%  # Handle missing values in Embarked
  step_dummy(all_nominal_predictors(), one_hot = FALSE) %>%
  step_lincomb(all_numeric_predictors())  

# For KNN recipes
knn_meanimpute <- recipe(Survived ~ ., data = train_data) %>% 
  step_nzv(all_predictors()) %>%  
  step_zv(all_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>%  
  step_unknown(Embarked) %>%  # Handle missing values in Embarked
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
  step_lincomb(all_numeric_predictors())  

knn_medianimpute <- recipe(Survived ~ ., data = train_data) %>% 
  step_nzv(all_predictors()) %>%  
  step_zv(all_predictors()) %>%
  step_impute_median(all_numeric_predictors()) %>%  
  step_unknown(Embarked) %>%  # Handle missing values in Embarked
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
  step_lincomb(all_numeric_predictors())  

knn_knnimpute <- recipe(Survived ~ ., data = train_data) %>% 
  step_nzv(all_predictors()) %>%  
  step_zv(all_predictors()) %>%
  step_impute_knn(all_numeric_predictors()) %>%  
  step_unknown(Embarked) %>%  # Handle missing values in Embarked
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
  step_lincomb(all_numeric_predictors())

```

Here, I've created six different preprocessing recipes to transform our raw data into model-ready format. Each recipe follows a similar pattern but with key differences in how missing values are handled:

1.   First, we remove near-zero and zero-variance predictors that don't contribute meaningful information

2.   Then we impute missing values using one of three strategies:

    -   Mean imputation (replacing missing values with the average)

    -   Median imputation (using the middle value, more robust to outliers)

    -   KNN imputation (using similar instances to estimate missing values)

3.   Next, we convert categorical variables to dummy variables

4.  Finally, we remove linearly dependent variables to avoid multicollinearity

The distinction between logistic regression and KNN recipes lies in the one-hot encoding: logistic regression uses regular dummy variables, while KNN benefits from one-hot encoding (setting `one_hot = TRUE`) due to its distance-based nature.

This comprehensive approach lets us compare different imputation strategies and determine which works best for our Titanic data.

# **7. Creating Model Lists and Workflow Sets**

```{r}
# Logistic Regression Preprocessors
logreg_preprocessors <- list(
  logreg_knn_impute = logreg_knnimpute,
  logreg_mean_impute = logreg_meanimpute,
  logreg_median_impute = logreg_medianimpute
)

# KNN Classification Preprocessors
knn_preprocessors <- list(
  knn_knn_impute = knn_knnimpute,
  knn_mean_impute = knn_meanimpute,
  knn_median_impute = knn_medianimpute
)

# Logistic Regression Model
logreg_models <- list(
  logreg_model = logistic_reg() |> 
    set_engine("glm") |> 
    set_mode("classification")
)

# KNN Models with different k values
knn_models <- list(
  knn5 = nearest_neighbor(neighbors = 5) |> 
    set_engine("kknn") |> 
    set_mode("classification"),
  
  knn10 = nearest_neighbor(neighbors = 10) |> 
    set_engine("kknn") |> 
    set_mode("classification")
)

# Create workflow sets for each model type
logreg_workflows <- workflow_set(logreg_preprocessors, logreg_models, cross = TRUE)
knn_workflows <- workflow_set(knn_preprocessors, knn_models, cross = TRUE)

# Combine all workflow sets
all_models <- bind_rows(logreg_workflows, knn_workflows)

# Print workflow set
all_models

```

With our preprocessing recipes and models defined, it's time to organize everything into a systematic framework like the Titanic in its early hours. I've structured our approach using workflow sets which is a powerful tidymodels feature that helps manage complex modeling processes.

I've grouped our preprocessing recipes by model type and created lists of our models with different configurations. Then, using `workflow_set()`, I've created all possible combinations of preprocessors and models. This gives us 9 different workflows:

-   3 logistic regression workflows (one for each imputation strategy)

-   6 KNN workflows (two different k values, each with three imputation strategies)

This approach keeps everything organized while automating the process of trying multiple model configurations. Rather than writing repetitive code for each combination, we now have a clean, methodical way to evaluate all our options.

# **8. Model Training and Evaluation**

```{r}
# Define classification metrics
titanic_class_metrics <- metric_set(accuracy, roc_auc)

# Fit all models using cross-validation
all_fits <- all_models |> 
  workflow_map("fit_resamples",
               resamples = titanic_folds,
               metrics = titanic_class_metrics)

# Collect metrics
collect_metrics(all_fits)

# Plot Accuracy for Classification Models
autoplot(all_fits, rank_metric = "accuracy") +
  geom_text_repel(aes(label = wflow_id), nudge_x = 1/8, nudge_y = 1/100) +
  theme(legend.position = "none")

# Plot AUC for Classification Models
autoplot(all_fits, rank_metric = "roc_auc") +
  geom_text_repel(aes(label = wflow_id), nudge_x = 1/8, nudge_y = 1/100) +
  theme(legend.position = "none")
```

Please be patient for this step, the Titanic wasn't built in a day.

Now for the exciting part, we will train and evaluate our models. Using the power of `workflow_map()`, I've fit all 9 model configurations across our 10-fold cross-validation with 10 repeats, effectively evaluating each model 100 times. This comprehensive approach gives us extremely reliable performance estimates.

I've focused on two key metrics:

-   Accuracy: the proportion of correct predictions

-   ROC AUC: a more nuanced measure of how well our models distinguish between survivors and non-survivors

The visualizations make it easy to compare performance across models. Interestingly, the top performers aren't necessarily what you might expect—while advanced methods like KNN have theoretical advantages, sometimes simpler models like logistic regression prove remarkably effective, especially with well-engineered features.

# **9. Selecting the Best Model and Making Predictions**

```{r}
# Select the best model
best_model <- all_fits |>
  rank_results(rank_metric = "roc_auc") |>
  filter(.metric == "roc_auc") |>
  slice(1) |>
  pull(wflow_id)

# Fit the best model on the full training data
final_fit <- all_models |>
  extract_workflow(best_model) |>
  fit(data = train_data)

# Make predictions on test data
test_results <- test_data |>
  mutate(
    pred_class = predict(final_fit, new_data = test_data)$.pred_class,
    pred_prob = predict(final_fit, new_data = test_data, type = "prob")$.pred_1
  )

# Create confusion matrix
conf_mat <- test_results |>
  conf_mat(truth = Survived, estimate = pred_class)

# Plot confusion matrix
conf_mat |>
  autoplot("heatmap")

# Calculate comprehensive metrics
binary_metrics <- metric_set(accuracy, recall, precision, 
                            specificity, npv)

test_metrics <- test_results |>
  binary_metrics(truth = Survived, estimate = pred_class, event_level = "second")

# Print metrics
test_metrics
```

After evaluating all contenders, I've selected our best-performing model based on ROC AUC score. This model balances multiple performance aspects rather than optimizing for a single metric like accuracy, which can be misleading when classes are imbalanced (as they are in the Titanic data).

I've trained this model on our full training dataset and deployed it against our test data which is the true measure of a model's predictive power. The confusion matrix visualization shows us where our model succeeds and struggles, while our comprehensive metrics suite gives us a multi-faceted view of performance:

-   Accuracy: Overall correct prediction rate

-   Recall/Sensitivity: Proportion of actual survivors correctly identified

-   Precision: Proportion of predicted survivors who actually survived

-   Specificity: Proportion of non-survivors correctly identified

-   Negative Predictive Value (NPV): Proportion of predicted non-survivors who actually didn't survive

# **10. ROC Curve Analysis**

```{r}
# Generate points for ROC curve
roc_curve_data <- test_results |>
  roc_curve(truth = Survived, pred_prob, event_level = "second")

# Plot ROC curve
roc_curve_data |>
  autoplot()

# Calculate AUC
auc_value <- test_results |>
  roc_auc(truth = Survived, pred_prob, event_level = "second")

# Print AUC
auc_value
```

A crucial aspect of binary classification models is understanding the tradeoff between sensitivity (catching true survivors) and specificity (correctly identifying non-survivors). The ROC curve brilliantly visualizes this tradeoff across different probability thresholds.

Our curve shows how the model performs at various decision thresholds, from being very strict about predicting survival (high specificity, low sensitivity) to being very permissive (high sensitivity, low specificity). The Area Under the Curve (AUC) gives us a single number summarizing the model's overall discriminative ability, irrespective of threshold choice.

An AUC close to 1.0 would indicate a near-perfect model, while 0.5 represents a model no better than random guessing. Our model's performance falls somewhere in between, highlighting both the strengths of our approach and the inherent unpredictability of survival in this historic disaster

## 13. Final Model Interpretation and Insights

Our journey through the Titanic dataset has revealed both technical insights about predictive modeling and sobering historical realities about the disaster itself.

From a modeling perspective, we've seen how different preprocessing strategies and algorithms perform, with our optimized model achieving respectable predictive accuracy. The comparison of logistic regression and KNN models demonstrates that sometimes simpler, more interpretable approaches can compete with more complex algorithms when features are well-engineered.

More importantly, our analysis has quantified what historical accounts have long described: survival was strongly influenced by socioeconomic factors and gender. First-class passengers had dramatically better odds than those in steerage, while women and children were prioritized during evacuation. These patterns reflect the social hierarchies and gender norms of early 20th century society, frozen in time by this maritime tragedy.

While no model can fully capture the chaos and individual stories of that fateful night, our analysis provides a data-driven window into who survived and why, illustrating how statistical methods can illuminate historical events.

# 11. Conclusion

This analysis has demonstrated the power of modern machine learning techniques for classification problems, even with historical datasets like the Titanic passenger records. By applying a systematic approach—from exploratory analysis through model selection, evaluation, and optimization—we've extracted meaningful insights and created a predictive model with respectable performance.

Beyond the technical aspects, this project serves as a reminder of how data science can illuminate historical events and social patterns. The stark differences in survival rates across passenger classes and demographic groups provide a quantitative lens on the human dimensions of this historic tragedy.

For future work, ensemble methods combining multiple models might further improve predictive performance, while additional feature engineering could extract even more signal from this rich historical dataset.
