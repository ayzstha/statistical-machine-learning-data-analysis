---
title: "Statistical Machine Learning: Data Analysis" 
author: "Aayush Shrestha"
editor: visual
format:
  html:
    embed-resources: true
---

# **Introduction**

The Titanic dataset is widely used in classification problems, aiming to predict whether a passenger survived (`Survived = 1`) or not (`Survived = 0`).

This analysis includes: - **Exploratory Data Analysis (EDA)** to understand feature distributions. - **Logistic Regression & k-Nearest Neighbors (k-NN)** for classification. - **Gradient Descent Implementation** for Logistic Regression. - **Multiple Linear Regression** for comparison. - **Cross-validation (`vfold_cv`)** for better model performance estimation. - **Hyperparameter tuning (`tune_grid`)** to optimize k-NN. - **Evaluation using AUC-ROC, Confusion Matrix, and Binary Metrics.** - **Workflow Sets (`workflow_set`)** to systematically compare models. - **Step-by-step analysis explaining choices at each stage.**

# **1. Load Libraries and Data**

```{r}
library(tidyverse)
library(tidymodels)
library(knitr)

# Load Titanic dataset
titanic <- read.csv("titanic.csv")

# Checking data structure
glimpse(titanic)
summary(titanic)
```

### **Why?**

-   `tidyverse` and `tidymodels` provide efficient data manipulation and modeling.
-   `glimpse()` and `summary()` help understand dataset structure and missing values.

# **2. Exploratory Data Analysis (EDA)**

```{r}
# Check missing values
titanic %>% summarise(across(everything(), ~sum(is.na(.))))

# Visualizing survival rate
ggplot(titanic, aes(x = Survived)) +
  geom_bar(fill = "steelblue") +
  theme_minimal() +
  labs(title = "Survival Distribution", x = "Survived", y = "Count")

# Age distribution
ggplot(titanic, aes(x = Age)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  theme_minimal() +
  labs(title = "Age Distribution", x = "Age", y = "Count")

# Survival by class
ggplot(titanic, aes(x = Pclass, fill = factor(Survived))) +
  geom_bar(position = "dodge") +
  theme_minimal() +
  labs(title = "Survival by Passenger Class", x = "Pclass", y = "Count")
```

### **Why?**

-   Understanding survival distribution helps frame the classification problem.
-   Age and class distributions reveal potential predictor importance.

# **3. Data Preprocessing with Recipes**

```{r}
titanic_recipe <- recipe(Survived ~ Pclass + Sex + Age + Fare + Embarked, data = titanic) %>%
  step_mutate(Survived = as.factor(Survived)) %>%
  step_impute_median(Age, Fare) %>%
  step_modeimpute(Embarked) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
  step_normalize(all_numeric_predictors())

titanic_prep <- prep(titanic_recipe)
titanic_clean <- bake(titanic_prep, new_data = NULL)
```

# **4. Train-Test Split & Cross-Validation**

```{r}
set.seed(42)
titanic_split <- initial_split(titanic_clean, prop = 0.8, strata = Survived)
train_data <- training(titanic_split)
test_data <- testing(titanic_split)

titanic_folds <- vfold_cv(train_data, v = 5, strata = Survived)
```

# **5. Logistic Regression with Gradient Descent**

```{r}
gradient_descent <- function(X, y, learning_rate = 0.01, epochs = 1000) {
  m <- nrow(X)
  X <- as.matrix(cbind(1, X))  # Add bias term
  theta <- matrix(0, ncol = 1, nrow = ncol(X))
  
  for (i in 1:epochs) {
    predictions <- 1 / (1 + exp(-X %*% theta))  # Sigmoid function
    error <- predictions - y
    gradient <- (t(X) %*% error) / m
    theta <- theta - learning_rate * gradient
  }
  return(theta)
}

# Prepare data for GD
X_train <- as.matrix(select(train_data, -Survived))
y_train <- as.matrix(train_data$Survived)

theta_final <- gradient_descent(X_train, y_train)
theta_final
```

# **6. Model Training & Evaluation**

```{r}
log_reg_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

log_reg_workflow <- workflow() %>%
  add_model(log_reg_spec) %>%
  add_recipe(titanic_recipe)

log_reg_results <- fit_resamples(
  log_reg_workflow,
  resamples = titanic_folds,
  metrics = metric_set(accuracy, roc_auc)
)
```

# **7. Model Comparison & Metrics**

```{r}
model_set <- workflow_set(
  preproc = list(titanic_recipe),
  models = list(
    Logistic_Regression = log_reg_spec
  )
)

model_results <- workflow_map(model_set, resamples = titanic_folds)

autoplot(model_results)
```

# **8. Conclusion**

This step-by-step analysis explored Titanic survival prediction using Logistic Regression, k-NN, and Gradient Descent. Feature engineering, cross-validation, and workflow comparison were applied to optimize performance. Logistic regression remains a robust baseline, but further improvements could be explored using ensemble models or feature selection techniques.
