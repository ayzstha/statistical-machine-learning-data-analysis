---
title: "Statistical Machine Learning: Data Analysis" 
author: "Aayush Shrestha"
editor: visual
format:
  html:
    embed-resources: true
---

# Introduction

In this project, I will analyze the E-Commerce Shipping Dataset from Kaggle. I will clean the data if necessary, explore trends, patterns, and anomalies, and present my findings in a clear and concise way. Through this project, I will showcase my skills in data manipulation, visualization, statistical inference, and basic statistical modeling. Additionally, it will highlight my familiarity with fundamental machine learning concepts and techniques, including regression and classification problems, data preprocessing, feature engineering, model building, and evaluation.

I will also be using the `tidyverse` and `tidymodels` libraries throughout the project.

## Data: E-Commerce Shipping Dataset

The dataset for this analysis comes from an international e-commerce company seeking key insights from its customer database. The company, which specializes in selling electronic products, aims to leverage advanced machine learning techniques to better understand its customers. The dataset, compiled by Prachi Gopalani on Kaggle, consists of 10,999 observations across 12 variables and serves as the foundation for model development.

\
The data contains the following information:

-   <div>

    -   **ID:** ID Number of Customers.

    -   **Warehouse block:** The Company have big Warehouse which is divided in to block such as A,B,C,D,E.

    -   **Mode of shipment:**The Company Ships the products in multiple way such as Ship, Flight and Road.

    -   **Customer care calls:** The number of calls made from enquiry for enquiry of the shipment.

    -   **Customer rating:** The company has rated from every customer. 1 is the lowest (Worst), 5 is the highest (Best).

    -   **Cost of the product:** Cost of the Product in US Dollars.

    -   **Prior purchases:** The Number of Prior Purchase.

    -   **Product importance:** The company has categorized the product in the various parameter such as low, medium, high.

    -   **Gender:** Male and Female.

    -   **Discount offered:** Discount offered on that specific product.

    -   **Weight in gms:** It is the weight in grams.

    -   **Reached on time:** It is the target variable, where 1 Indicates that the product has NOT reached on time and 0 indicates it has reached on time.

    </div>

My ultimate goal will be to predict `Reached on time` from the other features.

# Loading & Cleaning the Data

## Libraries

All the libraries required for the completion of the analysis:

```{r}
library(readr)
library(dplyr)
library(knitr)
library(tidyverse)
library(tidymodels)
library(caret)
library(ISLR2)
library(Stat2Data)
library(readODS)
library(ggrepel)

tidymodels_prefer()
```

## Step 1

::: {.callout-tip title="Question"}
The data are contained in `train.csv`, so we load the data.
:::

```{r}
data("Hawks")
str(Hawks)
```

## Exercise 2

::: {.callout-tip title="Question"}
Two of the variables in the data set shouldn't be useful because they just serve to identify the different LEGO sets. Which two are they? Remove them.
:::

The two variables that are not needed are the Item_number and Theme. Item_number is a unique integer used to denote the Lego set and Theme is used to identify the theme of the Lego set which in this case we do not need. Name of the Lego set should be good the identify what Lego set we are working on.

```{r}

lego <- lego |> select(-Item_Number, -Set_Name)
head(lego)
```

## Exercise 3

::: {.callout-tip title="Question"}
Notice that the `Weight` variable is a bit odd... It seems like it should be numeric but it's a `chr`. Why? Write code to extract the true numerical weight in either lbs or Kgs (your choice). You are encouraged to use the internet and generative AI to help you figure out how to do this. However, make sure you are able to explain your code once you are done.
:::

```{r}

library(stringr)

# Extract numerical weight values in kg and rename column
lego <- lego |> mutate(
  Weight = case_when(
    str_detect(Weight, "Kg") ~ as.numeric(str_extract(Weight, "[0-9]+\\.?[0-9]*")),  # Extract Kg value
    TRUE ~ NA_real_  # Keep NA values
  )
) |> rename(`Weight (kg)` = Weight)  # Rename column

# View the cleaned dataset
print(lego)

```

-   While looking at the dataset, we can see that while some of the weight is missing, some have weight in both kgs and lbs, with the units included, which is why it is saved as chr. I chose to keep the kg value and NA for missing values.

-   **`mutate()`**: Used to modify the `Weight` column.

-   **`case_when()`**: Checks different conditions and assigns appropriate values.

    -   **If the weight contains "Kg"** → Extracts the number before "Kg" using `str_extract()`.

    -   **If neither "Kg" nor "lb" is found** → Assigns `NA_real_` (keeps missing values as NA).

-   `"[0-9]+\\.?[0-9]*"` → Extracts the first numeric value (for kg).

-   **Renames the column** from `Weight` to `Weight (kg)` for clarity.

## Exercise 4

::: {.callout-tip title="Question"}
For each of the 12 features do the following:
:::

### Exercise 4.1

::: {.callout-tip title="Question"}
Identify if they are the correct data type. Are categorical variables coded as factors? Are the factor levels in the correct order if necessary? Are numerical variables coded as numbers? You will need to read descriptions of the data to make this determination.
:::

```{r}
glimpse(lego)
lego <- lego |>
  mutate(Ages = factor(Ages, levels = c("Ages_2+",
                                        "Ages_6+",
                                        "Ages_8+",
                                        "Ages_9+",
                                        "Ages_10+",
                                        "Ages_6-12")))
lego <- lego |>
  mutate(Packaging = factor(Packaging),
         Availability = factor(Availability),
         Size = factor(Size, levels = c("Small",
                                        "Big")),
         Theme = factor(Theme))
```

### Exercise 4.2

::: {.callout-tip title="Question"}
Identify any variables with missing values. Identify and then fix any variables for whom missing values (i.e. `NA`s) indicate something other than that the data is missing (there is at least one). Fill in this missing values appropriately.
:::

```{r}
lego |> 
  summarize(across(everything(), ~ sum(is.na(.)))) |> 
  pivot_longer(everything()) |> 
  filter(value > 0) |> 
  kable()

lego <- lego |>
  mutate(Minifigures = ifelse(is.na(Minifigures) & Theme %in% c("Architecture",
                                                               "DUPLO",
                                                               "LEGO Art",
                                                               "LEGO Super Mario",
                                                               "Technic"),
                                    0, Minifigures))
```

-   We see that theme, ages, pages, minifigures, packages, weight, availability, and size all contain missing values. We know, from the data decription, that the NA in minifigures could represent a 0 or a missing data. To better get an idea of which data is missing and which data is actually a 0, we can consider if the lego theme historically has had minifigures in the past. We know that, generally, technic, super mario, art, duplo, and architecture sets generally do not contain minifigures.

### Exercise 4.3

::: {.callout-tip title="Question"}
For all of the categorical variables, identify ones that you think may be problematic because they may have near-zero variance. Decide whether to remove them now, or remove them as part of your pre-processor. Make an argument for why your choice is appropriate.
:::

```{r}
nearZeroVar(lego, saveMetrics = TRUE) |> kable()
lego <- lego |>
  select(-Size)
```

-   From the visualization above, we see that packaging, availability, and size all have near zero variance. From this, I only decided to remove size, as this column had zero variance. Additionally, although other columns had near zero variance, they could provide additional information to our model. For example in retail, lego exclusives could be more expensive than retail.

### Exercise 4.4

::: {.callout-tip title="Question"}
For all of the categorical variables, identify ones that you think may be problematic because they have many categories that don't have a lot of observations and likely need to be "lumped". Decide whether to remove them now, or remove them as part of your pre-processor. Make an argument for why your choice is appropriate.
:::

```{r}
lego |> count(Theme) |> kable()
```

-   We see that theme has many categories, and that some of those categories have few observations. Instead of removing this column, we could instead lump rare categories together under "other". We could do this in the preprocessing, as this lumping would be performed for modeling purposes.

# Data Splitting & Preprocessing

## Exercise 5

::: {.callout-tip title="Question"}
Split your data into training and test sets. Use your own judgement to determine training to test split ratio. Make sure to set a seed.
:::

```{r}
set.seed(427)
lego_split <- initial_split(lego, prop = .75, strata = Amazon_Price)
lego_train <- training(lego_split)
lego_test <- testing(lego_split)
```

## Exercise 6

::: {.callout-tip title="Question"}
Generate at least three different recipes designed to be used with linear regression that treat preprocessing differently. Hint: you'll likely want to try out different missing value imputation or lumping strategies. It's also a good idea to include `step_lincolm`.
:::

```{r}
lm_knnimpute <- recipe(Amazon_Price ~ ., data = lego_train) |> 
  step_nzv(all_predictors()) |>  # remove zero or near-zero variable predictors
  step_impute_knn(all_numeric_predictors()) |>  # impute missing values using KNN
  step_unknown(all_nominal_predictors()) |>  # handle missing categorical values
  step_other(all_nominal_predictors(), threshold = 0.01, other = "Other") |> # lump rare categories
  step_dummy(all_nominal_predictors(), one_hot = FALSE) |>  # in general use one_hot unless doing linear regression
  step_corr(all_numeric_predictors(), threshold = 0.75) |> # adjusted correlation threshold
  step_lincomb(all_numeric_predictors()) # remove variables that have exact linear combinations
  
lm_meanimpute <- recipe(Amazon_Price ~ ., data = lego_train) |> 
  step_nzv(all_predictors()) |>  # remove zero or near-zero variable predictors
  step_impute_mean(all_numeric_predictors()) |>  # impute missing values using mean
  step_unknown(all_nominal_predictors()) |>  # handle missing categorical values
  step_other(all_nominal_predictors(), threshold = 0.01, other = "Other") |> # lump rare categories
  step_dummy(all_nominal_predictors(), one_hot = FALSE) |>  # in general use one_hot unless doing linear regression
  step_corr(all_numeric_predictors(), threshold = 0.75) |> # adjusted correlation threshold
  step_lincomb(all_numeric_predictors()) # remove variables that have exact linear combinations
  
lm_medianimpute <- recipe(Amazon_Price ~ ., data = lego_train) |> 
  step_nzv(all_predictors()) |>  # remove zero or near-zero variable predictors
  step_impute_median(all_numeric_predictors()) |>  # impute missing values using median
  step_unknown(all_nominal_predictors()) |>  # handle missing categorical values
  step_other(all_nominal_predictors(), threshold = 0.01, other = "Other") |> # lump rare categories
  step_dummy(all_nominal_predictors(), one_hot = FALSE) |>  # in general use one_hot unless doing linear regression
  step_corr(all_numeric_predictors(), threshold = 0.75) |> # adjusted correlation threshold
  step_lincomb(all_numeric_predictors()) # remove variables that have exact linear combinations
```

## Exercise 7

::: {.callout-tip title="Question"}
Generate at least three different recipes designed to be used with $K$-nearest neighbors that treat preprocessing differently. Hint: you'll likely want to try out different missing value imputation or lumping strategies.
:::

```{r}
knn_preproc1 <- recipe(Amazon_Price ~ ., data = lego_train) |> 
  step_nzv(all_predictors()) |>  # remove zero or near-zero variable predictors
  step_impute_knn(all_numeric_predictors()) |>  # impute missing values using KNN
  step_unknown(all_nominal_predictors()) |>  # handle missing categorical values
  step_other(all_nominal_predictors(), threshold = 0.01, other = "Other") |> # lump rare categories
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>  # one-hot encode categorical variables
  step_corr(all_numeric_predictors(), threshold = 0.75) |> # adjusted correlation threshold
  step_lincomb(all_numeric_predictors()) |> # remove linear combinations
  step_normalize(all_numeric_predictors()) # normalize numeric predictors
  
knn_preproc2 <- recipe(Amazon_Price ~ ., data = lego_train) |> 
  step_nzv(all_predictors()) |>  # remove zero or near-zero variable predictors
  step_impute_mean(all_numeric_predictors()) |>  # impute missing values using mean
  step_unknown(all_nominal_predictors()) |>  # handle missing categorical values
  step_other(all_nominal_predictors(), threshold = 0.01, other = "Other") |> # lump rare categories
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>  # one-hot encode categorical variables
  step_corr(all_numeric_predictors(), threshold = 0.75) |> # adjusted correlation threshold
  step_lincomb(all_numeric_predictors()) |> # remove linear combinations
  step_normalize(all_numeric_predictors()) # normalize numeric predictors
  
knn_preproc3 <- recipe(Amazon_Price ~ ., data = lego_train) |> 
  step_nzv(all_predictors()) |>  # remove zero or near-zero variable predictors
  step_impute_median(all_numeric_predictors()) |>  # impute missing values using median
  step_unknown(all_nominal_predictors()) |>  # handle missing categorical values
  step_other(all_nominal_predictors(), threshold = 0.01, other = "Other") |> # lump rare categories
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>  # one-hot encode categorical variables
  step_corr(all_numeric_predictors(), threshold = 0.75) |> # adjusted correlation threshold
  step_lincomb(all_numeric_predictors()) |> # remove linear combinations
  step_normalize(all_numeric_predictors()) # normalize numeric predictors
```

# Model-Fitting & Evaluation

## Exercise 7

::: {.callout-tip title="Question"}
Create a `workflow_set` that contains 12 different workflows:

-   three linear regression workflows: one linear regression model with each of the three recipes you created above
-   nine different KNN workflows: choose three different $K$s for you KNN models and create one workflow for each combination of KNN model and preprocessing recipe
:::

```{r}
# Linear regression model
lm_model <- linear_reg() |> set_engine("lm")

# KNN models with different K values
knn5_model <- nearest_neighbor(mode = "regression", neighbors = 5) |> set_engine("kknn")
knn10_model <- nearest_neighbor(mode = "regression", neighbors = 10) |> set_engine("kknn")
knn15_model <- nearest_neighbor(mode = "regression", neighbors = 15) |> set_engine("kknn")

# Define preprocessing strategies for KNN
knn_preprocessors <- list(
  knn_knn_impute = knn_preproc1,
  knn_mean_impute = knn_preproc2,
  knn_median_impute = knn_preproc3
)

# Define preprocessing strategies for Linear Regression
lm_preprocessors <- list(
  lm_knn_impute = lm_knnimpute,
  lm_mean_impute = lm_meanimpute,
  lm_median_impute = lm_medianimpute
)

# Associate models with preprocessing strategies
knn_models <- list(
  knn5 = knn5_model,
  knn10 = knn10_model,
  knn15 = knn15_model
)

lm_models <- list(
  lm_model = lm_model
)

# Create workflow sets
knn_workflows <- workflow_set(knn_preprocessors, knn_models, cross = TRUE)
lm_workflows <- workflow_set(lm_preprocessors, lm_models, cross = TRUE)

# Combine workflows
all_models <- bind_rows(lm_workflows, knn_workflows)

print(all_models)  # Check workflows

```

## Exercise 8

::: {.callout-tip title="Question"}
Use 5 fold CV with 5 repeats to compute the RMSE and R-squared for each of the 12 workflows you created above. Note that this step may take a few minutes to execute.
:::

```{r}
# Define metrics
lego_metrics <- metric_set(rmse, rsq)

# Define 5-fold CV with 5 repeats
lego_folds <- vfold_cv(lego_train, v = 5, repeats = 5)

# Fit resamples for all models
all_fits <- all_models |> 
  workflow_map("fit_resamples",
               resamples = lego_folds,
               metrics = lego_metrics)

# Collect and display RMSE results
collect_metrics(all_fits) |> 
  filter(.metric == "rmse") |> 
  kable()

# Collect and display R-squared results
collect_metrics(all_fits) |> 
  filter(.metric == "rsq") |> 
  kable()

```

## Exercise 9

::: {.callout-tip title="Question"}
Plot the results of your cross validation and select your best workflow.
:::

```{r}
autoplot(all_fits, metric = "rmse") +
  geom_text_repel(aes(label = wflow_id), nudge_x = 1/8, nudge_y = 1/100, angle = 90) +
  theme(legend.position = "none")

```

```{r}
autoplot(all_fits, metric = "rsq") +
  geom_text_repel(aes(label = wflow_id), nudge_x = 1/8, nudge_y = 1/100, angle = 90) +
  theme(legend.position = "none")

```

As we know, higher $R^2$ values and lower RMSE values are better.

As we can clearly see in the graph, lm models are clearly better than the KNN models. In the first graph lm models have the lowest RMSE values and in the second graph we have the highest $R^2$ values. Therefore linear model are better performers than LNN model in this scenario.

\
To be specific we observed that **lm_knn_impute_lm_model** to show the best performance out of all.

## Exercise 10

::: {.callout-tip title="Question"}
Re-fit your best model on the whole training set and estimate your error metrics on the test set.
:::

```{r}
best_workflow <- all_models |>
  extract_workflow("lm_knn_impute_lm_model")

final_fit <- best_workflow |>
  fit(data = lego_train)

final_predict <- final_fit |>
  predict(new_data = lego_test) |>
  bind_cols(lego_test)

test_metrics <- final_predict |>
  metrics(truth = Amazon_Price, estimate = .pred)

test_metrics |> kable()
```

# Conceptual Question

## Exercise 11 (Sample interview question)

::: {.callout-tip title="Question"}
The time to complete cross-validation can be substantially improved by using parallel processing. Below is the output for the copilot prompt "Generate pseudo-code in R to do cross-validation with repetition and multiple models". Which parts of this code can be run in parallel and which can't. Note any changes that you might need to make for this to be parallelizable.

```{r}
#| eval: FALSE

# Define the number of folds (k) and the number of repetitions (r)
k <- 5
r <- 3

# Define the list of models to evaluate
models <- list(
    model1 = train_model1,
    model2 = train_model2,
    model3 = train_model3
)

# Initialize a list to store the performance metrics for each model
all_performance_metrics <- list()

# Loop through each model
for (model_name in names(models)) {
    # Initialize a list to store the performance metrics for this model
    model_performance_metrics <- list()
    
    # Loop through each repetition
    for (rep in 1:r) {
        # Create k-fold cross-validation indices for this repetition
        folds <- createFolds(dataset$target_variable, k = k)
        
        # Initialize a list to store the performance metrics for this repetition
        performance_metrics <- list()
        
        # Loop through each fold
        for (i in 1:k) {
            # Use the i-th fold as the validation set
            validation_indices <- folds[[i]]
            validation_set <- dataset[validation_indices, ]
            
            # Use the remaining folds as the training set
            training_set <- dataset[-validation_indices, ]
            
            # Train the model on the training set
            model <- models[[model_name]](training_set)
            
            # Evaluate the model on the validation set
            performance <- evaluate_model(model, validation_set)
            
            # Store the performance metric
            performance_metrics[[i]] <- performance
        }
        
        # Store the performance metrics for this repetition
        model_performance_metrics[[rep]] <- performance_metrics
    }
    
    # Store the performance metrics for this model
    all_performance_metrics[[model_name]] <- model_performance_metrics
}

# Calculate the average performance metric for each model across all repetitions
average_performance <- sapply(all_performance_metrics, function(metrics) mean(unlist(metrics)))

# Output the average performance for each model
print("Average Performance for each model:")
print(average_performance)
```
:::
