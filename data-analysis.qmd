---
title: "Statistical Machine Learning: Data Analysis" 
author: "Aayush Shrestha"
editor: visual
format:
  html:
    embed-resources: true
---

# **Introduction**

The Titanic dataset from Kaggle (<https://www.kaggle.com/datasets/yasserh/titanic-dataset>) is a widely used dataset, often used for classification problems. It contains detailed information about passengers on the RMS Titanic, including attributes such as **age, gender, ticket class, fare, and survival status**. The goal of this project is to analyze models with survival status as the target variable. I will apply techniques such as logistic regression, k-nearest neighbors (KNN), gradient descent, and multiple linear regression while incorporating essential machine learning practices like cross-validation, hyperparameter tuning, and model evaluation. I will also conduct exploratory data analysis (EDA) to identify trends and patterns within the dataset, to make sure that our models are built on meaningful insights.

Beyond predictive modeling, this project reflects key responsibilities in a professional data science role. The workflow includes collecting, cleaning, and analyzing data, ensuring data accuracy and integrity, and developing visual reports and dashboards to track model performance. The ability to identify trends, anomalies, and key relationships within the data is crucial for making informed decisions. This analysis relies on industry-standard tools in R, using tidyverse for data manipulation and visualization and tidymodels for machine learning workflows. A strong grasp of statistical inference, regression and classification models, preprocessing, feature engineering, and model evaluation is necessary for building interpretable models. By following a structured and methodical approach, this project not only enhances my technical expertise but also reinforces best practices in data science and machine learning.

# **1. Load Libraries and Data**

```{r}
library(tidyverse)
library(tidymodels)
library(knitr)

# Avoid any problems
tidymodels_prefer()

# Load Titanic dataset
titanic <- read.csv("titanic.csv")

# Checking data structure
glimpse(titanic)
summary(titanic)
```

The first step in data analysis is to actually load the data which is done with this step. I will also load all the libraries that are needed for this project to avoid redundancy. Let's also analyze the summary and see if there is anything meaningful here.

The Titanic dataset contains 891 passenger records, with `Survived` as the target variable (0 = did not survive, 1 = survived). The dataset includes numerical and categorical features such as `Pclass` (passenger class), `Age` (ranging from 0.42 to 80, with 177 missing values), `Fare` (ranging from 0 to 512.33), and `SibSp`/`Parch` (family aboard). Most passengers traveled alone, and survival rates indicate class-based disparities. Proper data preprocessing, including handling missing values and encoding categorical variables, is essential before model training.

# **2. Exploratory Data Analysis (EDA)**

```{r}
# Check missing values
titanic |> summarise(across(everything(), ~sum(is.na(.))))

# Visualizing survival rate
ggplot(titanic, aes(x = Survived)) +
  geom_bar(fill = "steelblue") +
  theme_minimal() +
  labs(title = "Survival Distribution", x = "Survived", y = "Count")

# Age distribution
ggplot(titanic, aes(x = Age)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  theme_minimal() +
  labs(title = "Age Distribution", x = "Age", y = "Count")

# Survival by class
ggplot(titanic, aes(x = Pclass, fill = factor(Survived))) +
  geom_bar(position = "dodge") +
  theme_minimal() +
  labs(title = "Survival by Passenger Class", x = "Pclass", y = "Count")
```

We understand a lot from these simple graphs. If we ignore some errors within the data, which we will address in our next step, we see that a lot more people did not survive as compared to those who did. If we dive deeper (no pun intended), we can also see that middle-aged people within the age of 20-40 had the highest chance of survival when compared to other ages. Another interesting yet not shocking graph reveals that the lowest third class passengers had the highest number of people who did not survive.

These simple yet effective graphs along with the summary give us a lot to work with but also highlight that there are a few anomalies within the data that need addressing. This is why we preprocess our data in the next step.

# **3. Clean Data**

```{r}
titanic <- titanic |> 
  mutate(
    Survived = as_factor(Survived),
    Pclass = factor(Pclass, levels = c(1, 2, 3), labels = c("First", "Second", "Third")),
    Sex = as_factor(Sex),
    Embarked = if_else(is.na(Embarked), "S", Embarked),  # Fill missing with most common
    Embarked = as_factor(Embarked),
    Has_Cabin = if_else(is.na(Cabin), 0, 1),  # Binary cabin feature
    Age = if_else(is.na(Age), median(Age, na.rm = TRUE), Age) # Impute missing Age
  ) |> 
  select(-Name, -Ticket, -Cabin)  # Remove Name, Cabin and Ticket columns

```

To preprocess the Titanic dataset effectively, we first converted categorical variables like `Survived`, `Pclass`, `Sex`, and `Embarked` into factors to ensure proper handling by machine learning models. Since `Pclass` represents an ordinal variable, we explicitly labeled its levels for better interpretability. Missing values in `Embarked` were replaced with the most common value, while `Cabin`, which had too many missing entries, was transformed into a binary `Has_Cabin` feature instead of direct imputation. The `Ticket` column, being highly inconsistent with mixed formats, was dropped as it lacked immediate predictive value. `Name` is also dropped because there are already other identifiers who do the same job. Finally, `Cabin` is also dropped since it doesn't necessarily add anything since we already have `Pclass` which is more valuable for us.

Further, missing values in `Age` and `Fare` will be handled in the preprocessing stage through imputation techniques to maintain data integrity.

I'm also imputing missing values in the `Age` column using the median to make things slightly simpler for myself instead of using model based imputation.

The next step is to split our data into training data and test data.

# **4. Train-Test Split & Cross-Validation**

```{r}
set.seed (401)
# "The Titanic is associated with the number 401 because that was the yard number assigned to the ship by its builders, Harland and Wolff". Just thought this was a fun seed number!

titanic_split <- initial_split(titanic, prop = 0.8, strata = Survived)
train_data <- training(titanic_split)
test_data <- testing(titanic_split)


titanic_folds <- vfold_cv(train_data, v = 10, strata = Survived, repeats = 10)
```

We set the seed to 401 as a nod to the Titanic's yard number. The dataset is split into an 80-20 train-test split while preserving the class distribution of `Survived` using stratification.

To improve model reliability, we also apply 10-fold cross-validation with 10 repeats on the training data, ensuring robust performance evaluation by reducing variance in model assessment.

Our next step will be to define our required models.

# **5. Define our models**

```{r}
# Logistic Regression Model
log_reg_model <- logistic_reg() |>
  set_engine("glm") |>
  set_mode("classification")

# Linear Regression (for potential numerical analysis, not classification)
lm_model <- linear_reg() |> 
  set_engine("lm")

# K-Nearest Neighbors (5 neighbors)
knn5_model <- nearest_neighbor(neighbors = 5) |>
  set_engine("kknn") |>
  set_mode("classification")

# K-Nearest Neighbors (10 neighbors)
knn10_model <- nearest_neighbor(neighbors = 10) |>
  set_engine("kknn") |>
  set_mode("classification")
```

Here, I have defined three models: Logistic Regression Model, Linear Regression Model and K-Nearest Neighbours (k =5 and k = 10). Since I am predicting Survived, which is a binary classification problem, my models are set to `"classification"` mode rather than `"regression"`.

Now, let us preprocess our data using recipes.

# **6. Preprocessing data using Recipes**

```{r}
# Logistic Regression - Mean Imputation
logreg_meanimpute <- recipe(Survived ~ ., data = train_data) |> 
  step_nzv(all_predictors()) |>  
  step_zv(all_predictors()) |>
  step_impute_mean(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors(), one_hot = FALSE) |>  
  step_corr(all_numeric_predictors(), threshold = 0.5) |>  
  step_lincomb(all_numeric_predictors())  

# Logistic Regression - Median Imputation
logreg_medianimpute <- recipe(Survived ~ ., data = train_data) |> 
  step_nzv(all_predictors()) |>  
  step_zv(all_predictors()) |>
  step_impute_median(all_numeric_predictors()) |>  
  step_dummy(all_nominal_predictors(), one_hot = FALSE) |>  
  step_corr(all_numeric_predictors(), threshold = 0.5) |>  
  step_lincomb(all_numeric_predictors())  

# Logistic Regression - KNN Imputation
logreg_knnimpute <- recipe(Survived ~ ., data = train_data) |> 
  step_nzv(all_predictors()) |> 
  step_zv(all_predictors()) |>
  step_impute_knn(all_numeric_predictors()) |>  
  step_dummy(all_nominal_predictors(), one_hot = FALSE) |>  
  step_corr(all_numeric_predictors(), threshold = 0.5) |>  
  step_lincomb(all_numeric_predictors())  

# KNN Classification - Mean Imputation
knn_meanimpute <- recipe(Survived ~ ., data = train_data) |> 
  step_nzv(all_predictors()) |>  
  step_zv(all_predictors()) |>
  step_impute_mean(all_numeric_predictors()) |>  
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>  
  step_corr(all_numeric_predictors(), threshold = 0.5) |>  
  step_lincomb(all_numeric_predictors())  

# KNN Classification - Median Imputation
knn_medianimpute <- recipe(Survived ~ ., data = train_data) |> 
  step_nzv(all_predictors()) |>  
  step_zv(all_predictors()) |>
  step_impute_median(all_numeric_predictors()) |>  
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>  
  step_corr(all_numeric_predictors(), threshold = 0.5) |>  
  step_lincomb(all_numeric_predictors())  

# KNN Classification - KNN Imputation
knn_knnimpute <- recipe(Survived ~ ., data = train_data) |> 
  step_nzv(all_predictors()) |>  
  step_zv(all_predictors()) |>
  step_impute_knn(all_numeric_predictors()) |>  
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>  
  step_corr(all_numeric_predictors(), threshold = 0.5) |>  
  step_lincomb(all_numeric_predictors())  

# Linear Regression - Mean Imputation
lm_meanimpute <- recipe(Survived ~ ., data = train_data) |> 
  step_nzv(all_predictors()) |> 
  step_zv(all_predictors()) |>
  step_impute_mean(all_numeric_predictors()) |>  
  step_dummy(all_nominal_predictors(), one_hot = FALSE) |>  
  step_corr(all_numeric_predictors(), threshold = 0.5) |>  
  step_lincomb(all_numeric_predictors())  

# Linear Regression - Median Imputation
lm_medianimpute <- recipe(Survived ~ ., data = train_data) |> 
  step_nzv(all_predictors()) |> 
  step_zv(all_predictors()) |>
  step_impute_median(all_numeric_predictors()) |>  
  step_dummy(all_nominal_predictors(), one_hot = FALSE) |>  
  step_corr(all_numeric_predictors(), threshold = 0.5) |>  
  step_lincomb(all_numeric_predictors())  

# Linear Regression - KNN Imputation
lm_knnimpute <- recipe(Survived ~ ., data = train_data) |> 
  step_nzv(all_predictors()) |>  
  step_zv(all_predictors()) |>
  step_impute_knn(all_numeric_predictors()) |>  
  step_dummy(all_nominal_predictors(), one_hot = FALSE) |>  
  step_corr(all_numeric_predictors(), threshold = 0.5) |>  
  step_lincomb(all_numeric_predictors())  

```

With this, each model has three versions of preprocessing using KNN, Mean and Median. However, this looks a bit too messy and all over the place. To organize everything for my sake and yours, I will arrange this into model lists and workflow sets.

# **7. Model lists and Workflow Sets**

```{r}
# Logistic Regression Preprocessors
logreg_preprocessors <- list(
  logreg_knn_impute = logreg_knnimpute,
  logreg_mean_impute = logreg_meanimpute,
  logreg_median_impute = logreg_medianimpute
)

# KNN Classification Preprocessors
knn_preprocessors <- list(
  knn_knn_impute = knn_knnimpute,
  knn_mean_impute = knn_meanimpute,
  knn_median_impute = knn_medianimpute
)

# Linear Regression Preprocessors
lm_preprocessors <- list(
  lm_knn_impute = lm_knnimpute,
  lm_mean_impute = lm_meanimpute,
  lm_median_impute = lm_medianimpute
)

# Logistic Regression Model
logreg_models <- list(
  logreg_model = logistic_reg() |> 
    set_engine("glm") |> 
    set_mode("classification")
)

# KNN Models with different k values
knn_models <- list(
  knn5 = nearest_neighbor(neighbors = 5) |> 
    set_engine("kknn") |> 
    set_mode("classification"),
  
  knn10 = nearest_neighbor(neighbors = 10) |> 
    set_engine("kknn") |> 
    set_mode("classification")
)

# Linear Regression Model
lm_models <- list(
  lm_model = linear_reg() |> 
    set_engine("lm")
)

# Create workflow sets for each model type
logreg_workflows <- workflow_set(logreg_preprocessors, logreg_models, cross = TRUE)
knn_workflows <- workflow_set(knn_preprocessors, knn_models, cross = TRUE)
lm_workflows <- workflow_set(lm_preprocessors, lm_models, cross = TRUE)

# Combine all workflow sets
all_models <- bind_rows(logreg_workflows, knn_workflows, lm_workflows)

# Print workflow set
all_models

```

Now everything looks very organized and put together, much like the Titanic in its early hours. Now we have 12 components to look at. Let's collect metrics and create a plot to better understand our preprocessing.

```{r}
valid_fits <- all_fits %>%
  filter(map_lgl(result, ~ length(.x) > 0))

collect_metrics(valid_fits)

```

# **8. Metrics and Plotting**

```{r}
# Define classification and regression metrics
titanic_class_metrics <- metric_set(accuracy, roc_auc)
titanic_reg_metrics <- metric_set(rmse, rsq)

# Fit resamples using cross-validation
all_fits <- all_models |> 
  workflow_map("fit_resamples",
               resamples = titanic_folds,
               metrics = titanic_class_metrics) # Use titanic_reg_metrics for LM

# Plot Accuracy for Classification Models
autoplot(all_fits, metric = "accuracy") +
  geom_text_repel(aes(label = wflow_id), nudge_x = 1/8, nudge_y = 1/100) +
  theme(legend.position = "none")

# Plot AUC for Classification Models
autoplot(all_fits, metric = "roc_auc") +
  geom_text_repel(aes(label = wflow_id), nudge_x = 1/8, nudge_y = 1/100) +
  theme(legend.position = "none")

# Plot RMSE for Regression Models
autoplot(all_fits, metric = "rmse") +
  geom_text_repel(aes(label = wflow_id), nudge_x = 1/8, nudge_y = 1/100) +
  theme(legend.position = "none")

# Plot R-squared for Regression Models
autoplot(all_fits, metric = "rsq") +
  geom_text_repel(aes(label = wflow_id), nudge_x = 1/8, nudge_y = 1/100) +
  theme(legend.position = "none")
```

# **8. Conclusion**

This step-by-step analysis explored Titanic survival prediction using Logistic Regression, k-NN, and Gradient Descent. Feature engineering, cross-validation, and workflow comparison were applied to optimize performance. Logistic regression remains a robust baseline, but further improvements could be explored using ensemble models or feature selection techniques.
